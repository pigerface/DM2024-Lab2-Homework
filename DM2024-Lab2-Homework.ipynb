{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: é»ƒç„å½°\n",
    "\n",
    "Student ID: 110030031\n",
    "\n",
    "GitHub ID: pigerface@gmail.com\n",
    "\n",
    "Kaggle name:\n",
    "\n",
    "Kaggle private scoreboard snapshot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from transformers import BertTokenizer\n",
    "    \n",
    "\n",
    "\n",
    "def load_data(emotion_path, data_identification_path, tweets_path):\n",
    "    \"\"\"\n",
    "    è¼‰å…¥èˆ‡è™•ç†æ•¸æ“šçš„å‡½æ•¸\n",
    "    :param emotion_path: str, emotion.csv æª”æ¡ˆè·¯å¾‘\n",
    "    :param data_identification_path: str, data_identification.csv æª”æ¡ˆè·¯å¾‘\n",
    "    :param tweets_path: str, tweets_DM.json æª”æ¡ˆè·¯å¾‘\n",
    "    :return: pd.DataFrame, pd.DataFrame, pd.DataFrame\n",
    "    \"\"\"\n",
    "    # è®€å– emotion å’Œ data_identification æ•¸æ“š\n",
    "    emotion = pd.read_csv(emotion_path)\n",
    "    data_identification = pd.read_csv(data_identification_path)\n",
    "\n",
    "    # è®€å– tweets_DM.json ä¸¦è§£æç‚º DataFrame\n",
    "    with open(tweets_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    _source = df['_source'].apply(lambda x: x['tweet'])\n",
    "    df = pd.DataFrame({\n",
    "        'tweet_id': _source.apply(lambda x: x['tweet_id']),\n",
    "        'hashtags': _source.apply(lambda x: x['hashtags']),\n",
    "        'text': _source.apply(lambda x: x['text']),\n",
    "    })\n",
    "    \n",
    "    # åˆä½µ data_identification\n",
    "    df = df.merge(data_identification, on='tweet_id', how='left')\n",
    "\n",
    "    return df, emotion, data_identification\n",
    "\n",
    "\n",
    "def preprocess_data(df, emotion, label_encoder, tokenizer):\n",
    "    \"\"\"\n",
    "    è™•ç†æ•¸æ“šã€åˆ†å‰²è¨“ç·´èˆ‡æ¸¬è©¦é›†ï¼Œä¸¦å®Œæˆ Tokenization\n",
    "    \"\"\"\n",
    "    # Tokenization å‡½æ•¸\n",
    "    def tokenize_function(text):\n",
    "        return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "    # åˆ†å‰²æ•¸æ“šé›†\n",
    "    train_data = df[df['identification'] == 'train']\n",
    "    test_data = df[df['identification'] == 'test']\n",
    "\n",
    "    # åˆä½µ emotion èˆ‡è¨“ç·´é›†\n",
    "    train_data = train_data.merge(emotion, on='tweet_id', how='left')\n",
    "    train_data.drop_duplicates(subset=['text'], keep=False, inplace=True)\n",
    "\n",
    "    # åˆ†å‰²è¨“ç·´èˆ‡é©—è­‰é›†\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_data['text'], train_data['emotion'], test_size=0.2, random_state=42, stratify=train_data['emotion']\n",
    "    )\n",
    "\n",
    "    # Reset index\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    y_val.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Label Encoding\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_val = label_encoder.transform(y_val)\n",
    "    \n",
    "    # Tokenize è¨“ç·´èˆ‡é©—è­‰é›†\n",
    "    X_train_tokenized = X_train.apply(tokenize_function)\n",
    "    X_val_tokenized = X_val.apply(tokenize_function)\n",
    "    test_data_tokenized = test_data['text'].apply(tokenize_function)\n",
    "    \n",
    "\n",
    "    return train_data, test_data, X_train_tokenized, X_val_tokenized, y_train, y_val, test_data_tokenized\n",
    "\n",
    "def predict_and_generate_submission(trainer, label_encoder, test_dataset, output_path=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    æ ¹æ“šæ¸¬è©¦æ•¸æ“šç”Ÿæˆé æ¸¬ä¸¦ä¿å­˜ç‚ºKaggleæäº¤æ ¼å¼\n",
    "    :param trainer: Trainer object, å·²è¨“ç·´çš„æ¨¡å‹\n",
    "    :param test_dataset: pd.DataFrame, æ¸¬è©¦æ•¸æ“š\n",
    "    :param output_path: str, ä¿å­˜æäº¤æ–‡ä»¶çš„è·¯å¾‘\n",
    "    \"\"\"\n",
    "\n",
    "    # ä½¿ç”¨æ¨¡å‹é€²è¡Œé æ¸¬\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    predicted_classes = predictions.predictions.argmax(axis=1)\n",
    "    \n",
    "    predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
    "\n",
    "    # å°‡é æ¸¬çµæœæ·»åŠ åˆ°æ¸¬è©¦æ•¸æ“šä¸­\n",
    "    test_data['emotion'] = predicted_labels\n",
    "\n",
    "    # ä¿å­˜ç‚ºKaggleæäº¤æ ¼å¼\n",
    "    submission = test_data[['tweet_id', 'emotion']]\n",
    "    # å°‡tweet_idæ”¹æˆid\n",
    "    submission.rename(columns={'tweet_id': 'id'}, inplace=True)\n",
    "    submission.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Kaggleæäº¤æ–‡ä»¶å·²ä¿å­˜è‡³ {output_path}ï¼\")\n",
    "\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "                'input_ids': self.encodings['input_ids'][idx],\n",
    "                'attention_mask': self.encodings['attention_mask'][idx],\n",
    "                'labels': torch.tensor(self.labels[idx])\n",
    "            }\n",
    "        return item\n",
    "    \n",
    "def convert_to_dicts(tokenized_texts):\n",
    "    # ä¿®æ”¹è½‰æ›æ–¹å¼ï¼Œç¢ºä¿è¼¸å‡ºæ­£ç¢ºçš„å¼µé‡æ ¼å¼\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in tokenized_texts:\n",
    "        # ç§»é™¤å¤šé¤˜çš„ç¶­åº¦\n",
    "        input_ids.append(text['input_ids'].squeeze(0))\n",
    "        attention_masks.append(text['attention_mask'].squeeze(0))\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.stack(input_ids),\n",
    "        'attention_mask': torch.stack(attention_masks)\n",
    "    }\n",
    "\n",
    "# è¨ˆç®—è©•ä¼°æŒ‡æ¨™\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(axis=1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    precision = precision_score(labels, preds, average=\"weighted\")\n",
    "    recall = recall_score(labels, preds, average=\"weighted\")\n",
    "    return {\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shen/.local/share/virtualenvs/2024NTHU_DATAMINING-mXtGUlb_/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results', # All files generated during training will be stored here\n",
    "#     num_train_epochs=3, # The model will be trained for 3 full epochs unless the step limit (max_steps) is reached first\n",
    "#     per_device_train_batch_size=5, # Training batch size per device (GPU or CPU).\n",
    "#     per_device_eval_batch_size=5, # Evaluation batch size per device (GPU or CPU).\n",
    "#     warmup_steps=10, # Number of warm-up steps during which the learning rate gradually increases to its initial value\n",
    "#     weight_decay=0.01, # Weight decay rate: this technique helps to avoid overfitting, penalizing large weights in the neural network\n",
    "#     logging_dir='./logs', # Directory where training logs will be stored\n",
    "#     max_steps=10,  # Maximum number of training steps to be performed\n",
    "#     save_steps=2,  # Range of steps after which the model will be saved\n",
    "#     logging_steps=2,  # Range of steps after which log information will be recorded\n",
    "# )\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                  # å„²å­˜è¼¸å‡º\n",
    "    evaluation_strategy=\"steps\",            # æŒ‰æ­¥é€²è¡Œè©•ä¼°\n",
    "    eval_steps=500,                         # æ¯ 500 æ­¥é€²è¡Œä¸€æ¬¡è©•ä¼°\n",
    "    save_steps=500,                         # æ¯ 500 æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "    logging_steps=100,                      # æ¯ 100 æ­¥è¨˜éŒ„ä¸€æ¬¡æ—¥èªŒ\n",
    "    learning_rate=2e-5,                     # åˆå§‹å­¸ç¿’ç‡\n",
    "    per_device_train_batch_size=128,         # è¨“ç·´æ‰¹æ¬¡å¤§å°\n",
    "    per_device_eval_batch_size=128,          # è©•ä¼°æ‰¹æ¬¡å¤§å°\n",
    "    num_train_epochs=3,                     # è¨“ç·´ 3 å€‹é€±æœŸ\n",
    "    warmup_steps=500,                       # ç†±èº«æ­¥æ•¸\n",
    "    weight_decay=0.01,                      # L2 æ­£å‰‡åŒ–\n",
    "    load_best_model_at_end=True,            # è¼‰å…¥æœ€ä½³æ¨¡å‹\n",
    "    metric_for_best_model=\"f1\",             # ä½¿ç”¨ F1 åˆ¤å®šæœ€ä½³æ¨¡å‹\n",
    "    greater_is_better=True,                 # F1 è¶Šé«˜è¶Šå¥½\n",
    "    fp16=True                               # ä½¿ç”¨ 16-bit æµ®é»æ•¸åŠ é€Ÿ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•¸æ“šè¼‰å…¥èˆ‡è™•ç†å®Œæˆï¼\n",
      "è¨“ç·´é›†æ¨£æœ¬æ•¸ï¼š1159345ï¼Œé©—è­‰é›†æ¨£æœ¬æ•¸ï¼š289837\n",
      "Data preprocessing completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3500' max='27174' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3500/27174 36:13 < 4:05:06, 1.61 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.409400</td>\n",
       "      <td>1.337915</td>\n",
       "      <td>0.476073</td>\n",
       "      <td>0.541557</td>\n",
       "      <td>0.524036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.201600</td>\n",
       "      <td>1.181912</td>\n",
       "      <td>0.555444</td>\n",
       "      <td>0.597985</td>\n",
       "      <td>0.579060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.142900</td>\n",
       "      <td>1.118451</td>\n",
       "      <td>0.580705</td>\n",
       "      <td>0.612800</td>\n",
       "      <td>0.600372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.114000</td>\n",
       "      <td>1.082297</td>\n",
       "      <td>0.594780</td>\n",
       "      <td>0.624844</td>\n",
       "      <td>0.607800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.084500</td>\n",
       "      <td>1.058882</td>\n",
       "      <td>0.598449</td>\n",
       "      <td>0.638351</td>\n",
       "      <td>0.617095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.044000</td>\n",
       "      <td>1.049764</td>\n",
       "      <td>0.607919</td>\n",
       "      <td>0.625044</td>\n",
       "      <td>0.621211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.056500</td>\n",
       "      <td>1.034557</td>\n",
       "      <td>0.614092</td>\n",
       "      <td>0.630521</td>\n",
       "      <td>0.624517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shen/.local/share/virtualenvs/2024NTHU_DATAMINING-mXtGUlb_/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shen/.local/share/virtualenvs/2024NTHU_DATAMINING-mXtGUlb_/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "  - Loss: 4.1442\n",
      "  - Runtime: 250.57 seconds\n",
      "  - Samples per Second: 1644.13\n",
      "  - Steps per Second: 12.85\n",
      "  - Epoch: 0.3864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shen/.local/share/virtualenvs/2024NTHU_DATAMINING-mXtGUlb_/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipykernel_25048/2359703616.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['emotion'] = predicted_labels\n",
      "/tmp/ipykernel_25048/2359703616.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  submission.rename(columns={'tweet_id': 'id'}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggleæäº¤æ–‡ä»¶å·²ä¿å­˜è‡³ ./BERT_param2/submission.csvï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "# è¨­ç½®æ•¸æ“šè·¯å¾‘\n",
    "# data_base_path = '/kaggle/input/dm-2024-isa-5810-lab-2-homework/'\n",
    "data_base_path = './data/'\n",
    "emotion_path = data_base_path + 'emotion.csv'\n",
    "data_identification_path = data_base_path + 'data_identification.csv'\n",
    "tweets_path = data_base_path + 'tweets_DM.json'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# è¼‰å…¥èˆ‡è™•ç†æ•¸æ“š\n",
    "df, emotion, data_identification = load_data(emotion_path, data_identification_path, tweets_path)\n",
    "train_data, test_data, X_train, X_val, y_train, y_val, test_data_tokenized = preprocess_data(df, emotion, label_encoder, tokenizer)\n",
    "\n",
    "print(\"æ•¸æ“šè¼‰å…¥èˆ‡è™•ç†å®Œæˆï¼\")\n",
    "print(f\"è¨“ç·´é›†æ¨£æœ¬æ•¸ï¼š{len(X_train)}ï¼Œé©—è­‰é›†æ¨£æœ¬æ•¸ï¼š{len(X_val)}\")\n",
    "\n",
    "# Convert to lists of dictionaries\n",
    "train_encodings = convert_to_dicts(X_train)\n",
    "val_encodings = convert_to_dicts(X_val)\n",
    "test_encodings = convert_to_dicts(test_data_tokenized)\n",
    "\n",
    "assert len(train_encodings['input_ids']) == len(y_train), \"Encodings and labels must have the same length.\"\n",
    "assert len(val_encodings['input_ids']) == len(y_val), \"Encodings and labels must have the same length.\"\n",
    "print(\"Data preprocessing completed successfully.\")\n",
    "\n",
    "# Create three dataset objects using the SentimentDataset\n",
    "train_dataset = TweetDataset(train_encodings, y_train)\n",
    "val_dataset = TweetDataset(val_encodings, y_val)\n",
    "test_dataset = TweetDataset(test_encodings, labels=[0] * len(test_encodings['input_ids']))\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                          # æ¨¡å‹\n",
    "    args=training_args,                   # è¨“ç·´åƒæ•¸\n",
    "    train_dataset=train_dataset,          # è¨“ç·´è³‡æ–™\n",
    "    eval_dataset=val_dataset,             # é©—è­‰è³‡æ–™\n",
    "    compute_metrics=compute_metrics,       # è©•ä¼°æŒ‡æ¨™\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)\n",
    "    ]\n",
    ")\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the Model\n",
    "results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"  - Loss: {results['eval_loss']:.4f}\")\n",
    "print(f\"  - Runtime: {results['eval_runtime']:.2f} seconds\")\n",
    "print(f\"  - Samples per Second: {results['eval_samples_per_second']:.2f}\")\n",
    "print(f\"  - Steps per Second: {results['eval_steps_per_second']:.2f}\")\n",
    "print(f\"  - Epoch: {results['epoch']:.4f}\")\n",
    "\n",
    "# Save the model and tokenizer in the current folder\n",
    "model_save_path = \"./BERT_param2\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "submission_path = \"./BERT_param2/submission.csv\"\n",
    "predict_and_generate_submission(trainer, label_encoder, test_dataset, output_path=submission_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2024NTHU_DATAMINING-mXtGUlb_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
